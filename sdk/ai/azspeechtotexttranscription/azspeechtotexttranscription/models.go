// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.
// Code generated by Microsoft (R) Go Code Generator. DO NOT EDIT.

package azspeechtotexttranscription

import "time"

// ChannelCombinedPhrases - The full transcript per channel.
type ChannelCombinedPhrases struct {
	// REQUIRED; The complete transcribed text for the channel.
	Text *string

	// The 0-based channel index. Only present if channel separation is enabled.
	Channel *int32
}

// DiarizationProperties - Speaker Identification Properties
type DiarizationProperties struct {
	// REQUIRED; A value indicating whether speaker identification is enabled.
	Enabled *bool

	// REQUIRED; A hint for the maximum number of speakers for diarization. Must be greater than 1 and less than 36.
	MaxSpeakers *int32
}

// EntityError
type EntityError struct {
	// READ-ONLY; The code of this error.
	Code *string

	// READ-ONLY; The message for this error.
	Message *string
}

// EntityReference
type EntityReference struct {
	// REQUIRED; The location of the referenced entity.
	Self *string
}

// FileLinks
type FileLinks struct {
	// REQUIRED; The url to retrieve the content of this file.
	ContentURL *string
}

// FileProperties
type FileProperties struct {
	// REQUIRED; The total duration in milliseconds of the file in case this file is an audio file.
	DurationMilliseconds *int32

	// REQUIRED; The size of the data in bytes.
	Size *int32
}

// LanguageIdentificationProperties
type LanguageIdentificationProperties struct {
	// REQUIRED; The candidate locales for language identification (example ["en-US", "de-DE", "es-ES"]). A minimum of 2 and a
	// maximum of 10 candidate locales, including the main locale for the transcription, is supported for continuous mode. For
	// single language identification, the maximum number of candidate locales is unbounded.
	CandidateLocales []*string

	// The mode used for language identification.
	Mode *LanguageIdentificationMode

	// An optional mapping of locales to speech model entities. If no model is given for a locale, the default base model is used.
	// Keys must be locales contained in the candidate locales, values are entities for models of the respective locales.
	SpeechModelMapping map[string]*EntityReference
}

// SpeechToTextCustomPageTranscriptionFile - Page of entities.
type SpeechToTextCustomPageTranscriptionFile struct {
	// READ-ONLY; A link to the next set of paginated results if there are more entities available; otherwise null.
	NextLink *string

	// READ-ONLY; A list of entities limited by either the passed query parameters 'skip' and 'top' or their default values.
	//
	// When iterating through a list using pagination and deleting entities in parallel, some entities will be skipped in the
	// results.
	// It's recommended to build a list on the client and delete after the fetching of the complete list.
	Values []*TranscriptionFile
}

// SpeechToTextCustomPageTranscriptionJob - Page of entities.
type SpeechToTextCustomPageTranscriptionJob struct {
	// READ-ONLY; A link to the next set of paginated results if there are more entities available; otherwise null.
	NextLink *string

	// READ-ONLY; A list of entities limited by either the passed query parameters 'skip' and 'top' or their default values.
	//
	// When iterating through a list using pagination and deleting entities in parallel, some entities will be skipped in the
	// results.
	// It's recommended to build a list on the client and delete after the fetching of the complete list.
	Values []*TranscriptionJob
}

// TranscribeConfig - Metadata for a fast transcription request.
type TranscribeConfig struct {
	// The 0-based indices of the channels to be transcribed separately. If not specified, multiple channels are merged and transcribed
	// jointly. Only up to two channels are supported.
	ActiveChannels []*int32

	// Mode of diarization.
	DiarizationProperties *TranscribeDiarizationProperties

	// A list of possible locales for the transcription. If not specified, the locale of the speech in the audio is detected automatically
	// from all supported locales.
	Locales []*string

	// Maps some or all candidate locales to a model URI to be used for transcription. If no mapping is given, the default model
	// for the locale is used.
	Models map[string]*string

	// Mode of profanity filtering.
	ProfanityFilterMode *ProfanityFilterMode
}

// TranscribeDiarizationProperties - The Speaker Identification settings. Diarization settings must be specified to enable
// speaker identification.
type TranscribeDiarizationProperties struct {
	// Gets or sets a value indicating whether speaker diarization is enabled.
	Enabled *bool

	// Gets or sets a hint for the maximum number of speakers for diarization. Must be greater than 1 and less than 36.
	MaxSpeakers *int32
}

// TranscribeResult - The result of the transcribe operation.
type TranscribeResult struct {
	// REQUIRED; The full transcript for each channel.
	CombinedPhrases []*ChannelCombinedPhrases

	// REQUIRED; The duration of the audio in milliseconds.
	DurationMilliseconds *int32

	// REQUIRED; The transcription results segmented into phrases.
	Phrases []*TranscribedPhrase
}

// TranscribedPhrase - A transcribed phrase.
type TranscribedPhrase struct {
	// REQUIRED; The confidence value for the phrase.
	Confidence *float32

	// REQUIRED; The duration of the phrase in milliseconds.
	DurationMilliseconds *int32

	// REQUIRED; The start offset of the phrase in milliseconds.
	OffsetMilliseconds *int32

	// REQUIRED; The transcribed text of the phrase.
	Text *string

	// The 0-based channel index. Only present if channel separation is enabled.
	Channel *int32

	// The locale of the phrase.
	Locale *string

	// A unique integer number that is assigned to each speaker detected in the audio without particular order. Only present if
	// speaker diarization is enabled.
	Speaker *int32

	// The words that make up the phrase. Only present if word-level timestamps are enabled.
	Words []*TranscribedWord
}

// TranscribedWord - Time-stamped word in the display form.
type TranscribedWord struct {
	// REQUIRED; The duration of the word in milliseconds.
	DurationMilliseconds *int32

	// REQUIRED; The start offset of the word in milliseconds.
	OffsetMilliseconds *int32

	// REQUIRED; The recognized word, including punctuation.
	Text *string
}

// TranscriptionFile
type TranscriptionFile struct {
	// REQUIRED; The creation time of this file. The time stamp is encoded as ISO 8601 date and time format (see https://en.wikipedia.org/wiki/ISO_8601#Combined_date_and_time_representations).
	CreatedDateTime *time.Time

	// REQUIRED; The name of this file.
	DisplayName *string

	// REQUIRED; FileKind
	Kind *FileKind

	// REQUIRED; FileLinks
	Links *FileLinks

	// REQUIRED; FileProperties
	Properties *FileProperties

	// READ-ONLY; The location of this entity.
	Self *string
}

// TranscriptionJob - Transcription
type TranscriptionJob struct {
	// REQUIRED; The display name of the object.
	DisplayName *string

	// REQUIRED; The locale of the contained data. If Language Identification is used, this locale is used to transcribe speech
	// for which no language could be detected.
	Locale *string

	// REQUIRED; TranscriptionProperties
	Properties *TranscriptionProperties

	// READ-ONLY; The id of this entity.
	ID *string

	// READ-ONLY; The location of this entity.
	Self *string

	// READ-ONLY; The status of the object
	Status *JobStatus

	// A URL for an Azure blob container that contains the audio files. A container is allowed to have a maximum size of 5GB and
	// a maximum number of 10000 blobs.
	// The maximum size for a blob is 2.5GB.
	// Container SAS should contain 'r' (read) and 'l' (list) permissions.
	// This property will not be returned in a response.
	ContentContainerURL *string

	// A list of content urls to get audio files to transcribe. Up to 1000 urls are allowed.
	// This property will not be returned in a response.
	ContentUrls []*string

	// The custom properties of this entity. The maximum allowed key length is 64 characters, the maximum
	// allowed value length is 256 characters and the count of allowed entries is 10.
	CustomProperties map[string]*string

	// EntityReference
	Dataset *EntityReference

	// The description of the object.
	Description *string

	// TranscriptionLinks
	Links *TranscriptionLinks

	// EntityReference
	Model *EntityReference

	// READ-ONLY; The time-stamp when the object was created.
	// The time stamp is encoded as ISO 8601 date and time format
	// ("YYYY-MM-DDThh:mm:ssZ", see https://en.wikipedia.org/wiki/ISO_8601#Combined_date_and_time_representations).
	CreatedDateTime *time.Time

	// READ-ONLY; The time-stamp when the current status was entered.
	// The time stamp is encoded as ISO 8601 date and time format
	// ("YYYY-MM-DDThh:mm:ssZ", see https://en.wikipedia.org/wiki/ISO_8601#Combined_date_and_time_representations).
	LastActionDateTime *time.Time
}

// TranscriptionLinks
type TranscriptionLinks struct {
	// READ-ONLY; The location to get all files of this entity. See operation "Transcriptions_ListFiles" for more details.
	Files *string
}

// TranscriptionProperties
type TranscriptionProperties struct {
	// REQUIRED; How long the transcription will be kept in the system after it has completed. Once the transcription reaches
	// the time to live after completion(successful or failed) it will be automatically deleted.
	// Note: When using BYOS (bring your own storage), the result files on the customer owned storage account will also be deleted.Use
	// either destinationContainerUrl to specify a separate container for result files which will not be deleted when the timeToLive
	// expires, or retrieve the result files through the API and store them as needed.
	// The shortest supported duration is 6 hours, the longest supported duration is 31 days. 2 days (48 hours) is the recommended
	// default value when data is consumed directly.
	TimeToLiveHours *int32

	// A collection of the requested channel numbers. In the default case, the channels 0 and 1 are considered.
	Channels []*int32

	// The requested destination container.
	// Remarks
	// When a destination container is used in combination with a timeToLive, the metadata of a transcription will be deleted
	// normally, but the data stored in the destination container, including transcription results, will remain untouched, because
	// no delete permissions are required for this container.
	// To support automatic cleanup, either configure blob lifetimes on the container, or use "Bring your own Storage (BYOS)"
	// instead of destinationContainerUrl, where blobs can be cleaned up.
	DestinationContainerURL *string

	// Speaker Identification
	Diarization *DiarizationProperties

	// A value indicating whether word level timestamps for the display form are requested. The default value is false.
	DisplayFormWordLevelTimestampsEnabled *bool

	// EntityError
	Error *EntityError

	// LanguageIdentificationProperties
	LanguageIdentificationProperties *LanguageIdentificationProperties

	// Mode of profanity filtering.
	ProfanityFilterMode *ProfanityFilterMode

	// The mode used for punctuation.
	PunctuationMode *PunctuationMode

	// A value indicating whether word level timestamps are requested. The default value is false.
	WordLevelTimestampsEnabled *bool

	// READ-ONLY; The duration in milliseconds of the transcription.
	// Durations larger than 2^53-1 are not supported to ensure compatibility with JavaScript integers.
	DurationMilliseconds *int32
}
